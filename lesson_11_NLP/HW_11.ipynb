{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW 11\n",
    "\n",
    "1. Load the embeddings from Google embedding to train Imbd dataset. Use code the code we used ing class. Report accuracy using testing data and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /Users/ir177/opt/anaconda3/lib/python3.8/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/ir177/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_hub) (3.12.4)\r\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/ir177/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_hub) (1.18.5)\r\n",
      "Requirement already satisfied: six>=1.9 in /Users/ir177/opt/anaconda3/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/ir177/opt/anaconda3/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow_hub) (49.2.0.post20200714)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import shutil \n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Use data from problem 2. Train a new model where you allow the Google embedding to be trainable. Report training and testing accuracy, compare with the results of problem #1. It may take up to an hour to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load data from Tweets that references Twitter account of  Trump or Biden from May, 2020 (tweets.csv). This data is a small sample of such tweets. (small sample). \n",
    "- Load data into padnas\n",
    "- create separate pandas with full text-column. Assign a datatype = tf.string\n",
    "- Load full-text into a tensorflow dataset.  Keep maximum number of observations to be divisible by 32, drop the few obsevations in the end to make the number divisible by 32, we need to it mimic bactch_size =32 in the training data.\n",
    "- Predict the positivity of a tweet using the IMDB model estiamted in #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing FYI\n",
    "# def clean_tweet_text(t):\n",
    "#     t= t.replace('@realDonaldTrump', 'realDonaldTrump' )\n",
    "#     t=t.replace('@JoeBiden', 'JoeBiden')\n",
    "#     # replace tweet response:\n",
    "#     t=t.replace('RT', '')\n",
    "#     # drop hyperlinks\n",
    "#     import re\n",
    "#     t= re.sub(r\"http\\S+\", \"\", t)\n",
    "#     # drop twitter links\n",
    "#     t= re.sub(r\"@\\S+\", \"\", t)\n",
    "#     # drop hashtags\n",
    "#     t= re.sub(r\"#\\S+\", \"\", t)\n",
    "#     #drop non-alpha-numeric charecteros\n",
    "#     t= re.sub(r\"^[A-Za-z0-9.,]\", \"\", t)\n",
    "#     # Try to decode emojist using emoji names\n",
    "#     t= t.encode(sys.getdefaultencoding()).decode('utf-8')\n",
    "#     # drop extract space\n",
    "#     t = re.sub(' +', ' ', t).strip()\n",
    "#     return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing \n",
    "# import json\n",
    "# def tw_dic_clean(file, data):\n",
    "#     # load gzip file\n",
    "#     with gzip.open(file, 'r') as fin:\n",
    "#         json_list = list(fin)\n",
    "#     # loop over the tweets in JSON\n",
    "#     for json_str in json_list:\n",
    "#         # load tweet a dict\n",
    "#         result = json.loads(json_str)\n",
    "#         # check if R or D:\n",
    "#         R = False\n",
    "#         D = False\n",
    "#         keep = False\n",
    "#         for i in result['entities']['user_mentions']:\n",
    "#             if i['screen_name'] == 'realDonaldTrump':\n",
    "#                 R = True\n",
    "#                 keep = True\n",
    "#             if i['screen_name'] == 'JoeBiden':\n",
    "#                 D = True\n",
    "#                 keep = True\n",
    "#         # If a tweet mentions Trump or Biden then keep:\n",
    "#         if keep == True:\n",
    "#             clean_text = clean_tweet_text(result['full_text'])\n",
    "#             # keep tweets of at least 100 characters and non-mission location\n",
    "#             if len(clean_text) >= 100 and (len(result['user']['location'])>0):\n",
    "#                 #add entries from dict\n",
    "#                 add_dict = {}\n",
    "#                 add_dict['tweet_id']= result['id']\n",
    "#                 add_dict['user_screen_name']= result['user']['screen_name']\n",
    "#                 add_dict['full_text']= clean_text\n",
    "#                 add_dict['D'] = D\n",
    "#                 add_dict['R'] = R\n",
    "#                 add_dict['location']= result['user']['location']\n",
    "#                 add_dict['favorited']= result['favorited']\n",
    "#                 add_dict['retweeted']= result['retweeted']\n",
    "#                 add_dict['in_reply_to_screen_name']= result['in_reply_to_screen_name']\n",
    "#                 # add clean tweet to the list\n",
    "#                 data.append(add_dict)\n",
    "#                 del R, D\n",
    "#     return data\n",
    "\n",
    "\n",
    "\n",
    "  # data = []\n",
    "# search_path = 'datasets'\n",
    "# file_type = \".gz\"\n",
    "# # get all files\n",
    "# for fname in os.listdir(path=search_path):\n",
    "#     if fname.endswith(file_type):\n",
    "#         data = tw_dic_clean('datasets/' + fname, data)\n",
    "#         #with gzip.open('datasets/' + fname, 'r') as fin:\n",
    "#         #json_list = list(fin)\n",
    "        \n",
    "# save dict as csv\n",
    "# import csv\n",
    "# keys = data[0].keys()\n",
    "# with open('tweets.csv', 'w', newline='')  as output_file:\n",
    "#     dict_writer = csv.DictWriter(output_file, keys)\n",
    "#     dict_writer.writeheader()\n",
    "#     dict_writer.writerows(data)  \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Merged predicted probability of positive tweet back to the dataframe. \n",
    "Print:\n",
    "- top 3 tweets with the lowest positive (like) probability for Biden\n",
    "- top 3 tweets with the lowest positive (like) probability for Trump\n",
    "- top 3 tweets with the highest positive (like) probability for Biden\n",
    "- top 3 tweets with the highest positive (like) probability for Trump\n",
    "\n",
    "Keep only the tweets where only Trump or Biden appears, don't use when they appear both. Use R and D indicators for that, only one should be True and other should be False. The indicators are based on the twitter account, so I a person uses the name without reference of Twitter account this wo\n",
    "\n",
    "Some of the tweets are quite rude, try not to read them to closely. Remember this is just a class exerise for sentimenal analysis.\n",
    "\n",
    "What do you think about the quality of Twitter sentimental analysis model trained on the IMDB? What do you think it's strong and weak points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
